{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-outputs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT    = E:\\canfiles\\Competitions\\math_modeling\\2026_1_19_MCM_simulation\\Problem_B\\project\n",
      "OUT_DATA= E:\\canfiles\\Competitions\\math_modeling\\2026_1_19_MCM_simulation\\Problem_B\\project\\outputs\\data\n",
      "OUT_FIG = E:\\canfiles\\Competitions\\math_modeling\\2026_1_19_MCM_simulation\\Problem_B\\project\\outputs\\fig\n",
      "OUT_TAB = E:\\canfiles\\Competitions\\math_modeling\\2026_1_19_MCM_simulation\\Problem_B\\project\\outputs\\tab\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ===== Output directories (for figures/tables/data used in LaTeX) =====\n",
    "ROOT = Path.cwd()  # if this is not your project root, change it manually\n",
    "OUT_DIR = ROOT / 'outputs'\n",
    "OUT_DATA = OUT_DIR / 'data'\n",
    "OUT_FIG  = OUT_DIR / 'fig'\n",
    "OUT_TAB  = OUT_DIR / 'tab'\n",
    "\n",
    "OUT_DATA.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
    "OUT_TAB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('ROOT    =', ROOT)\n",
    "print('OUT_DATA=', OUT_DATA)\n",
    "print('OUT_FIG =', OUT_FIG)\n",
    "print('OUT_TAB =', OUT_TAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b224f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Task1 Result ==========\n",
      "FLOW_MODE = hall_calls\n",
      "Test MAE  = 4.7576\n",
      "Test RMSE = 8.6967\n",
      "Saved Task1 prediction series to: E:\\canfiles\\Competitions\\math_modeling\\2026_1_19_MCM_simulation\\Problem_B\\project\\outputs\\data\\task1_pred_series.csv\n",
      "Saved model to: task1_flow_model.joblib\n",
      "Next slice time: 2025-11-30 22:55:00\n",
      "Predicted flow : 0.3875\n"
     ]
    }
   ],
   "source": [
    "# Task1: Predict total passenger flow volume for the next 5-minute time slice\n",
    "# Based on cleaned CSVs:\n",
    "# /mnt/data/hall_calls_clean.csv\n",
    "# /mnt/data/load_changes_clean.csv\n",
    "# /mnt/data/car_calls_clean.csv\n",
    "# /mnt/data/car_stops_clean.csv\n",
    "# /mnt/data/car_departures_clean.csv\n",
    "# /mnt/data/maintenance_mode_clean.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"8\"  # 改成你电脑想用的核心数（比如 8）\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths\n",
    "# -----------------------------\n",
    "PATH_HALL = \"data/clean/hall_calls_clean.csv\"\n",
    "PATH_LOAD = \"data/clean/load_changes_clean.csv\"\n",
    "PATH_MAINT = \"data/clean/maintenance_mode_clean.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Config\n",
    "# -----------------------------\n",
    "FREQ = \"5min\"\n",
    "\n",
    "# 客流定义方式：\n",
    "# \"hall_calls\" = 每5分钟 hall call 数（推荐）\n",
    "# \"load_in\"    = 每5分钟 Load In 总重量 / avg_weight 估算人数（需要你确认单位与平均体重）\n",
    "FLOW_MODE = \"hall_calls\"\n",
    "\n",
    "# 仅在 FLOW_MODE=\"load_in\" 时使用：平均乘客体重（单位要与 Load In 一致）\n",
    "AVG_PASSENGER_WEIGHT = 65.0  # 例如 65 kg（如果 Load In 是 kg）；不确定请改成你们合理数值\n",
    "\n",
    "# 滞后特征：用过去多少个 5-min 片段\n",
    "LAGS = [1, 2, 3, 6, 12]  # 5/10/15/30/60分钟\n",
    "ROLL_WINDOWS = [3, 6, 12]  # rolling mean window size in slices\n",
    "\n",
    "# 训练集比例（按时间顺序切分）\n",
    "TRAIN_RATIO = 0.85\n",
    "\n",
    "MODEL_OUT = \"task1_flow_model.joblib\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Helpers\n",
    "# -----------------------------\n",
    "def add_time_features(df: pd.DataFrame, time_col=\"Time\") -> pd.DataFrame:\n",
    "    \"\"\"加入能适应一天不同时段的周期特征\"\"\"\n",
    "    out = df.copy()\n",
    "    t = out[time_col]\n",
    "\n",
    "    out[\"hour\"] = t.dt.hour\n",
    "    out[\"minute\"] = t.dt.minute\n",
    "    out[\"dow\"] = t.dt.dayofweek  # Monday=0\n",
    "\n",
    "    # 一天的分钟数 [0, 1440)\n",
    "    minute_of_day = out[\"hour\"] * 60 + out[\"minute\"]\n",
    "    # 周期编码：sin/cos（适应昼夜规律）\n",
    "    out[\"sin_day\"] = np.sin(2 * np.pi * minute_of_day / 1440.0)\n",
    "    out[\"cos_day\"] = np.cos(2 * np.pi * minute_of_day / 1440.0)\n",
    "\n",
    "    # 一周周期（可选）\n",
    "    out[\"sin_week\"] = np.sin(2 * np.pi * out[\"dow\"] / 7.0)\n",
    "    out[\"cos_week\"] = np.cos(2 * np.pi * out[\"dow\"] / 7.0)\n",
    "\n",
    "    # 是否工作日\n",
    "    out[\"is_weekend\"] = (out[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_lag_features(df: pd.DataFrame, y_col=\"y\") -> pd.DataFrame:\n",
    "    \"\"\"滞后 + 滚动统计特征\"\"\"\n",
    "    out = df.copy()\n",
    "    for lag in LAGS:\n",
    "        out[f\"lag_{lag}\"] = out[y_col].shift(lag)\n",
    "\n",
    "    for w in ROLL_WINDOWS:\n",
    "        out[f\"roll_mean_{w}\"] = out[y_col].shift(1).rolling(w).mean()\n",
    "        out[f\"roll_std_{w}\"] = out[y_col].shift(1).rolling(w).std()\n",
    "\n",
    "    # 简单趋势：最近1小时与前1小时差（如果有足够数据）\n",
    "    if 12 in LAGS:\n",
    "        out[\"trend_1h\"] = out[\"lag_1\"] - out[\"lag_12\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_flow_series(freq=FREQ, mode=FLOW_MODE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    构造 5分钟总客流时间序列。\n",
    "    mode:\n",
    "      - hall_calls: 每5min hall_calls数量\n",
    "      - load_in: 每5min Load In/avg_weight 估算“进电梯人数”\n",
    "    \"\"\"\n",
    "    if mode == \"hall_calls\":\n",
    "        hall = pd.read_csv(PATH_HALL)\n",
    "        hall[\"Time\"] = pd.to_datetime(hall[\"Time\"])\n",
    "        # 5分钟聚合：数量\n",
    "        s = hall.set_index(\"Time\").resample(freq).size().rename(\"y\").to_frame()\n",
    "\n",
    "    elif mode == \"load_in\":\n",
    "        load = pd.read_csv(PATH_LOAD)\n",
    "        load[\"Time\"] = pd.to_datetime(load[\"Time\"])\n",
    "        # 5分钟聚合：Load In 总重量 -> 估计人数\n",
    "        s = load.set_index(\"Time\")[\"Load In\"].resample(freq).sum().fillna(0)\n",
    "        s = (s / AVG_PASSENGER_WEIGHT).rename(\"y\").to_frame()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"FLOW_MODE must be 'hall_calls' or 'load_in'\")\n",
    "\n",
    "    # 补齐缺失时间片\n",
    "    s = s.asfreq(freq, fill_value=0).reset_index().rename(columns={\"Time\": \"Time\"})\n",
    "    return s\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Train + Evaluate + Predict\n",
    "# -----------------------------\n",
    "def train_task1():\n",
    "    # 3.1 Build target series\n",
    "    df = build_flow_series()\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "\n",
    "    # 3.2 Add features\n",
    "    df = add_time_features(df, \"Time\")\n",
    "    df = add_lag_features(df, \"y\")\n",
    "\n",
    "    # 3.3 Drop rows with NaNs due to lag/rolling\n",
    "    df_feat = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"sin_day\", \"cos_day\", \"sin_week\", \"cos_week\", \"is_weekend\",\n",
    "        \"hour\", \"minute\",\n",
    "    ] + [f\"lag_{l}\" for l in LAGS] + \\\n",
    "        [f\"roll_mean_{w}\" for w in ROLL_WINDOWS] + \\\n",
    "        [f\"roll_std_{w}\" for w in ROLL_WINDOWS] + ([\"trend_1h\"] if \"trend_1h\" in df_feat.columns else [])\n",
    "\n",
    "    X = df_feat[feature_cols].values\n",
    "    y = df_feat[\"y\"].values\n",
    "\n",
    "    # 3.4 Time-based split\n",
    "    n = len(df_feat)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_test, y_test = X[n_train:], y[n_train:]\n",
    "\n",
    "    # 3.5 Model\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        loss=\"squared_error\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=400,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3.6 Evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "    print(\"========== Task1 Result ==========\")\n",
    "    print(f\"FLOW_MODE = {FLOW_MODE}\")\n",
    "    print(f\"Test MAE  = {mae:.4f}\")\n",
    "    print(f\"Test RMSE = {rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # 3.6b Export series for report figures (test window)\n",
    "    try:\n",
    "        test_time = df_feat[\"Time\"].iloc[n_train:].reset_index(drop=True)\n",
    "        task1_pred_df = pd.DataFrame({\n",
    "            \"Time\": test_time,\n",
    "            \"y_true\": y_test,\n",
    "            \"y_pred\": pred\n",
    "        })\n",
    "        out_csv = str(OUT_DATA / \"task1_pred_series.csv\")\n",
    "        task1_pred_df.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved Task1 prediction series to: {out_csv}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Task1 export failed:\", e)\n",
    "\n",
    "# 3.7 Save\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"freq\": FREQ,\n",
    "        \"flow_mode\": FLOW_MODE,\n",
    "        \"avg_passenger_weight\": AVG_PASSENGER_WEIGHT,\n",
    "        \"lags\": LAGS,\n",
    "        \"roll_windows\": ROLL_WINDOWS\n",
    "    }\n",
    "    joblib.dump(payload, MODEL_OUT)\n",
    "    print(f\"Saved model to: {MODEL_OUT}\")\n",
    "\n",
    "    # 3.8 Predict next 5-min slice using the latest available time slice\n",
    "    next_pred, next_time = predict_next(df, payload)\n",
    "    print(f\"Next slice time: {next_time}\")\n",
    "    print(f\"Predicted flow : {next_pred:.4f}\")\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "def predict_next(raw_df: pd.DataFrame, payload: dict):\n",
    "    \"\"\"\n",
    "    raw_df: dataframe with columns [Time, y] at 5min freq (may include other cols)\n",
    "    \"\"\"\n",
    "    model = payload[\"model\"]\n",
    "    feature_cols = payload[\"feature_cols\"]\n",
    "\n",
    "    df = raw_df.copy()\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "\n",
    "    # Rebuild features the same way\n",
    "    df = add_time_features(df, \"Time\")\n",
    "    df = add_lag_features(df, \"y\")\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Take last row as \"current time slice\", then we predict next one by creating next time row\n",
    "    last = df.iloc[-1:].copy()\n",
    "    last_time = last[\"Time\"].iloc[0]\n",
    "    next_time = last_time + pd.Timedelta(FREQ)\n",
    "\n",
    "    # Create a new row for next_time with y unknown, but lag features come from history\n",
    "    next_row = pd.DataFrame({\"Time\": [next_time], \"y\": [np.nan]})\n",
    "    next_row = add_time_features(next_row, \"Time\")\n",
    "\n",
    "    # Build lag/rolling features for next time based on existing y series\n",
    "    y_series = df.set_index(\"Time\")[\"y\"].copy()\n",
    "\n",
    "    # Fill lag features\n",
    "    for lag in LAGS:\n",
    "        next_row[f\"lag_{lag}\"] = y_series.iloc[-lag] if len(y_series) >= lag else np.nan\n",
    "\n",
    "    # Fill rolling features\n",
    "    for w in ROLL_WINDOWS:\n",
    "        vals = y_series.iloc[-w:] if len(y_series) >= w else y_series\n",
    "        next_row[f\"roll_mean_{w}\"] = vals.mean() if len(vals) > 0 else np.nan\n",
    "        next_row[f\"roll_std_{w}\"] = vals.std() if len(vals) > 1 else 0.0\n",
    "\n",
    "    if \"trend_1h\" in feature_cols:\n",
    "        if len(y_series) >= 12:\n",
    "            next_row[\"trend_1h\"] = y_series.iloc[-1] - y_series.iloc[-12]\n",
    "        else:\n",
    "            next_row[\"trend_1h\"] = 0.0\n",
    "\n",
    "    # Ensure all required features exist\n",
    "    for c in feature_cols:\n",
    "        if c not in next_row.columns:\n",
    "            next_row[c] = 0.0\n",
    "\n",
    "    X_next = next_row[feature_cols].values\n",
    "    pred_next = model.predict(X_next)[0]\n",
    "    return float(pred_next), str(next_time)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_task1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5224e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DATA = E:\\canfiles\\Competitions\\math_modeling\\2026_1_19_MCM_simulation\\Problem_B\\project\\outputs\\data\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot auto-find your 5-min features DataFrame. Please set `features_df = <your_dataframe>` manually in this cell.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot auto-find your 5-min features DataFrame. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set `features_df = <your_dataframe>` manually in this cell.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Ensure Time column exists\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m features_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# try to infer time column\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot auto-find your 5-min features DataFrame. Please set `features_df = <your_dataframe>` manually in this cell."
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Cell 3 (REPLACEMENT): Task 2 mode clustering + naming + exports\n",
    "# Fixes KeyError 'activity_w' by being robust to feature column names.\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "PATH_HALL = \"data/clean/hall_calls_clean.csv\"\n",
    "PATH_CAR_CALL = \"data/clean/car_calls_clean.csv\"\n",
    "PATH_CAR_STOP = \"data/clean/car_stops_clean.csv\"\n",
    "PATH_CAR_DEP = \"data/clean/car_departures_clean.csv\"\n",
    "PATH_LOAD = \"data/clean/load_changes_clean.csv\"\n",
    "PATH_MAINT = \"data/clean/maintenance_mode_clean.csv\"\n",
    "\n",
    "# ---- make sure output dirs exist ----\n",
    "try:\n",
    "    OUT_DATA\n",
    "except NameError:\n",
    "    ROOT = Path.cwd()\n",
    "    OUT_DATA = ROOT / \"outputs\" / \"data\"\n",
    "    OUT_DATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"OUT_DATA =\", OUT_DATA)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0) Locate your per-5min feature table\n",
    "# You should already have a DataFrame in memory that aggregates to 5min slices.\n",
    "# In the patched notebook, it is usually named something like:\n",
    "#   feat, features_5min, mode_features, task3_mode_features_5min, etc.\n",
    "# We'll try to find it automatically; if it fails, you set it manually once.\n",
    "# -------------------------------------------------------------------\n",
    "candidate_names = [\n",
    "    \"feat\", \"features_5min\", \"mode_features\", \"mode_features_df\",\n",
    "    \"task3_mode_features_5min\", \"task3_features_5min\", \"feat_5min\"\n",
    "]\n",
    "\n",
    "features_df = None\n",
    "for n in candidate_names:\n",
    "    if n in globals() and isinstance(globals()[n], pd.DataFrame):\n",
    "        features_df = globals()[n]\n",
    "        print(\"Using features DataFrame:\", n, \"shape=\", features_df.shape)\n",
    "        break\n",
    "\n",
    "if features_df is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot auto-find your 5-min features DataFrame. \"\n",
    "        \"Please set `features_df = <your_dataframe>` manually in this cell.\"\n",
    "    )\n",
    "\n",
    "# Ensure Time column exists\n",
    "if \"Time\" not in features_df.columns:\n",
    "    # try to infer time column\n",
    "    for c in [\"time\", \"timestamp\", \"Datetime\", \"datetime\"]:\n",
    "        if c in features_df.columns:\n",
    "            features_df = features_df.rename(columns={c: \"Time\"})\n",
    "            break\n",
    "\n",
    "if \"Time\" not in features_df.columns:\n",
    "    raise KeyError(\"Your features table must contain a Time column (or time/timestamp/datetime).\")\n",
    "\n",
    "features_df = features_df.copy()\n",
    "features_df[\"Time\"] = pd.to_datetime(features_df[\"Time\"])\n",
    "features_df = features_df.sort_values(\"Time\").reset_index(drop=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Choose feature columns robustly (only those that exist)\n",
    "# We prefer interpretable columns if present; otherwise fall back to numeric columns.\n",
    "# -------------------------------------------------------------------\n",
    "preferred = [\n",
    "    # intensity\n",
    "    \"hall_calls_w\", \"hall_calls\", \"activity_w\", \"activity\",\n",
    "    # direction\n",
    "    \"up_ratio_w\", \"up_ratio\", \"down_ratio_w\", \"down_ratio\",\n",
    "    # dispersion / inter-floor\n",
    "    \"entropy_w\", \"entropy\", \"car_inter_ratio_w\", \"car_inter_ratio\", \"inter_ratio_w\", \"inter_ratio\",\n",
    "    # service pressure\n",
    "    \"stops_w\", \"stops\", \"departures_w\", \"departures\",\n",
    "    # load/maintenance\n",
    "    \"net_load_w\", \"net_load\", \"maint_ratio_w\", \"maint_ratio\",\n",
    "    # time cyclical (if you already built them here)\n",
    "    \"sin_day\", \"cos_day\", \"sin_week\", \"cos_week\"\n",
    "]\n",
    "feature_cols = [c for c in preferred if c in features_df.columns]\n",
    "\n",
    "# if too few, fall back to numeric columns (excluding obvious non-features)\n",
    "if len(feature_cols) < 4:\n",
    "    num_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    drop_like = {\"cluster\", \"mode_id\"}\n",
    "    num_cols = [c for c in num_cols if c not in drop_like]\n",
    "    feature_cols = num_cols[:10]  # keep compact\n",
    "    print(\"Fallback numeric feature_cols:\", feature_cols)\n",
    "else:\n",
    "    print(\"Using feature_cols:\", feature_cols)\n",
    "\n",
    "if len(feature_cols) < 2:\n",
    "    raise RuntimeError(\"Not enough numeric features found for clustering.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Fit KMeans on standardized features\n",
    "# -------------------------------------------------------------------\n",
    "X = features_df[feature_cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "# align Time index\n",
    "valid_idx = X.index\n",
    "X = X.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xz = scaler.fit_transform(X)\n",
    "\n",
    "K = 6  # your chosen number of clusters\n",
    "kmeans = KMeans(n_clusters=K, n_init=20, random_state=42)\n",
    "clusters = kmeans.fit_predict(Xz)\n",
    "\n",
    "# silhouette (only if K>1 and enough samples)\n",
    "sil = silhouette_score(Xz, clusters) if (K > 1 and len(np.unique(clusters)) > 1 and len(Xz) > K) else np.nan\n",
    "print(f\"Silhouette score: {sil:.4f}\" if not np.isnan(sil) else \"Silhouette score: N/A\")\n",
    "\n",
    "# write back clusters to full features_df (NaN rows keep cluster as NaN)\n",
    "features_df[\"cluster\"] = np.nan\n",
    "features_df.loc[valid_idx, \"cluster\"] = clusters\n",
    "features_df[\"cluster\"] = features_df[\"cluster\"].astype(\"Int64\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Interpretable naming: robust to missing columns\n",
    "# We map clusters -> labels using a few key statistics if available.\n",
    "# -------------------------------------------------------------------\n",
    "def col_any(df, names):\n",
    "    \"\"\"Return first existing column name from list, else None.\"\"\"\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "# pick key columns if present\n",
    "col_activity = col_any(features_df, [\"hall_calls_w\", \"hall_calls\", \"activity_w\", \"activity\"])\n",
    "col_up = col_any(features_df, [\"up_ratio_w\", \"up_ratio\"])\n",
    "col_down = col_any(features_df, [\"down_ratio_w\", \"down_ratio\"])\n",
    "col_entropy = col_any(features_df, [\"entropy_w\", \"entropy\"])\n",
    "col_inter = col_any(features_df, [\"car_inter_ratio_w\", \"car_inter_ratio\", \"inter_ratio_w\", \"inter_ratio\"])\n",
    "\n",
    "summary_cols = [c for c in [col_activity, col_up, col_down, col_entropy, col_inter] if c is not None]\n",
    "summary = features_df.dropna(subset=[\"cluster\"]).groupby(\"cluster\")[summary_cols].mean().reset_index()\n",
    "\n",
    "# thresholds based on quantiles to avoid hard-coded magic numbers\n",
    "def q(series, p):\n",
    "    return float(np.nanquantile(series.values, p))\n",
    "\n",
    "labels = {}\n",
    "if col_activity is not None:\n",
    "    a25, a50, a75 = q(summary[col_activity], 0.25), q(summary[col_activity], 0.50), q(summary[col_activity], 0.75)\n",
    "else:\n",
    "    a25 = a50 = a75 = None\n",
    "\n",
    "for _, row in summary.iterrows():\n",
    "    k = int(row[\"cluster\"])\n",
    "    act = float(row[col_activity]) if col_activity is not None else np.nan\n",
    "    up = float(row[col_up]) if col_up is not None else np.nan\n",
    "    down = float(row[col_down]) if col_down is not None else np.nan\n",
    "    ent = float(row[col_entropy]) if col_entropy is not None else np.nan\n",
    "    inter = float(row[col_inter]) if col_inter is not None else np.nan\n",
    "\n",
    "    # default\n",
    "    lab = \"Balanced\"\n",
    "\n",
    "    # Idle / Low activity\n",
    "    if col_activity is not None and act <= a25:\n",
    "        lab = \"Idle/Low\"\n",
    "\n",
    "    # Peaks (directional) if direction columns exist\n",
    "    if (col_up is not None) and (col_activity is not None) and act >= a75 and up >= 0.65:\n",
    "        lab = \"Up-Peak\"\n",
    "    if (col_down is not None) and (col_activity is not None) and act >= a75 and down >= 0.65:\n",
    "        lab = \"Down-Peak\"\n",
    "\n",
    "    # Inter-floor / dispersed patterns\n",
    "    if (col_inter is not None) and inter >= 0.55 and (col_activity is None or act >= a50):\n",
    "        lab = \"Inter-floor\"\n",
    "    if (col_entropy is not None) and ent >= q(summary[col_entropy], 0.75) and (col_activity is None or act >= a50):\n",
    "        # if very dispersed, label as mixed\n",
    "        if lab == \"Balanced\":\n",
    "            lab = \"Mixed/Dispersed\"\n",
    "\n",
    "    labels[k] = lab\n",
    "\n",
    "features_df[\"mode_label\"] = features_df[\"cluster\"].map(labels).fillna(\"Unknown\")\n",
    "\n",
    "print(\"Cluster->Label mapping:\", labels)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Exports required by writing + plotting scripts\n",
    "# 4.1 modes timeline\n",
    "# 4.2 mode features 5-min (with cluster + label)\n",
    "# -------------------------------------------------------------------\n",
    "timeline = features_df[[\"Time\", \"cluster\", \"mode_label\"]].copy()\n",
    "timeline_path = OUT_DATA / \"task2_modes_timeline.csv\"\n",
    "timeline.to_csv(timeline_path, index=False)\n",
    "print(\"Saved:\", timeline_path)\n",
    "\n",
    "feat_out = features_df.copy()\n",
    "feat_out_path = OUT_DATA / \"task3_mode_features_5min.csv\"\n",
    "feat_out.to_csv(feat_out_path, index=False)\n",
    "print(\"Saved:\", feat_out_path)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Build and export cluster-floor demand distribution w_f(m)\n",
    "# This requires hall_calls_clean with columns Time + Floor (or similar).\n",
    "# We try to locate your hall calls DF if it already exists; otherwise load from PATH_HALL if defined.\n",
    "# -------------------------------------------------------------------\n",
    "hall_df = None\n",
    "for n in [\"hall\", \"hall_df\", \"hall_calls\", \"hall_calls_df\", \"df_hall\"]:\n",
    "    if n in globals() and isinstance(globals()[n], pd.DataFrame):\n",
    "        hall_df = globals()[n]\n",
    "        print(\"Using hall calls DataFrame:\", n, \"shape=\", hall_df.shape)\n",
    "        break\n",
    "\n",
    "if hall_df is None:\n",
    "    # try to load from a path variable if present\n",
    "    path_hall = None\n",
    "    for n in [\"PATH_HALL\", \"HALL_PATH\", \"path_hall\", \"hall_path\"]:\n",
    "        if n in globals():\n",
    "            path_hall = globals()[n]\n",
    "            break\n",
    "    if path_hall is None:\n",
    "        print(\"WARNING: cannot find hall calls DataFrame or path; skip task3_demand_cluster_floor export.\")\n",
    "    else:\n",
    "        hall_df = pd.read_csv(path_hall)\n",
    "        print(\"Loaded hall calls from:\", path_hall, \"shape=\", hall_df.shape)\n",
    "\n",
    "if hall_df is not None:\n",
    "    hall_df = hall_df.copy()\n",
    "    # infer Time and Floor columns\n",
    "    if \"Time\" not in hall_df.columns:\n",
    "        for c in [\"time\", \"timestamp\", \"Datetime\", \"datetime\"]:\n",
    "            if c in hall_df.columns:\n",
    "                hall_df = hall_df.rename(columns={c: \"Time\"})\n",
    "                break\n",
    "    if \"Floor\" not in hall_df.columns:\n",
    "        for c in [\"floor\", \"FromFloor\", \"from_floor\", \"OriginFloor\", \"origin_floor\"]:\n",
    "            if c in hall_df.columns:\n",
    "                hall_df = hall_df.rename(columns={c: \"Floor\"})\n",
    "                break\n",
    "\n",
    "    if \"Time\" not in hall_df.columns or \"Floor\" not in hall_df.columns:\n",
    "        print(\"WARNING: hall calls must have Time and Floor (or synonyms). Skip export.\")\n",
    "    else:\n",
    "        hall_df[\"Time\"] = pd.to_datetime(hall_df[\"Time\"])\n",
    "        hall_df[\"Floor\"] = hall_df[\"Floor\"].astype(int)\n",
    "\n",
    "        # assign each call to 5-min slice start\n",
    "        hall_df[\"slice_time\"] = hall_df[\"Time\"].dt.floor(\"5min\")\n",
    "\n",
    "        # join slice -> cluster (mode)\n",
    "        slice_cluster = features_df[[\"Time\", \"cluster\"]].copy()\n",
    "        slice_cluster = slice_cluster.rename(columns={\"Time\": \"slice_time\"})\n",
    "        hall_df = hall_df.merge(slice_cluster, on=\"slice_time\", how=\"left\")\n",
    "\n",
    "        cf = hall_df.dropna(subset=[\"cluster\"]).groupby([\"cluster\", \"Floor\"]).size().reset_index(name=\"count\")\n",
    "        # normalize within cluster\n",
    "        cf[\"weight\"] = cf[\"count\"] / cf.groupby(\"cluster\")[\"count\"].transform(\"sum\")\n",
    "\n",
    "        dist_path = OUT_DATA / \"task3_demand_cluster_floor.csv\"\n",
    "        cf[[\"cluster\", \"Floor\", \"weight\"]].to_csv(dist_path, index=False)\n",
    "        print(\"Saved:\", dist_path, \"clusters=\", cf[\"cluster\"].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ac7657",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (2450366802.py, line 648)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 648\u001b[1;36m\u001b[0m\n\u001b[1;33m    return results\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Task3: Dynamic Parking Strategy Model (with a lightweight, reproducible simulator)\n",
    "# ------------------------------------------------------------\n",
    "# Uses your cleaned logs to:\n",
    "# 1) Learn demand-by-floor patterns conditioned on \"building mode\" (from Task2-style features)\n",
    "# 2) Build a Dynamic Parking Policy (k-median on floors with predicted demand weights)\n",
    "# 3) Simulate and compare 3 strategies:\n",
    "#    - \"last_stop\": do nothing (idle stays where it is)\n",
    "#    - \"lobby\": send idle elevators to lobby (floor=1)\n",
    "#    - \"dynamic\": mode-aware k-median parking floors + assignment\n",
    "#\n",
    "# Outputs:\n",
    "# - Average Waiting Time (AWT) and % Long Waits (>=60s) for each strategy\n",
    "#\n",
    "# Files used (cleaned CSV):\n",
    "# /mnt/data/hall_calls_clean.csv\n",
    "# /mnt/data/car_stops_clean.csv\n",
    "# /mnt/data/car_departures_clean.csv\n",
    "# /mnt/data/load_changes_clean.csv\n",
    "# /mnt/data/maintenance_mode_clean.csv\n",
    "# /mnt/data/car_calls_clean.csv  (optional here; not needed for basic simulation)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "PATH_HALL = \"data/clean/hall_calls_clean.csv\"\n",
    "PATH_STOP = \"data/clean/car_stops_clean.csv\"\n",
    "PATH_DEP  = \"data/clean/car_departures_clean.csv\"\n",
    "PATH_LOAD = \"data/clean/load_changes_clean.csv\"\n",
    "PATH_MAINT= \"data/clean/maintenance_mode_clean.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Config (tune if you want)\n",
    "# -----------------------------\n",
    "N_ELEVATORS = 8\n",
    "LOBBY_FLOOR = 1\n",
    "\n",
    "# \"Realistic-ish\" travel parameters (you can tune)\n",
    "SECONDS_PER_FLOOR = 1.5     # travel time per floor\n",
    "DOOR_TIME = 8.0             # open + dwell + close\n",
    "LONG_WAIT_THRESHOLD = 60.0  # seconds\n",
    "\n",
    "# Parking decision cadence / prediction horizon\n",
    "DECISION_FREQ = \"5min\"\n",
    "PRED_HORIZON_MIN = 15       # demand for next 15 minutes to choose parking\n",
    "\n",
    "# Cluster count for \"mode\" discovery\n",
    "N_CLUSTERS = 6\n",
    "\n",
    "# -----------------------------\n",
    "# Robust column pickers\n",
    "# -----------------------------\n",
    "def pick_col(df, candidates, required=False):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"Missing required column. Tried: {candidates}\")\n",
    "    return None\n",
    "\n",
    "def normalize_direction(series):\n",
    "    \"\"\"Return +1 (Up), -1 (Down), 0 (Unknown).\"\"\"\n",
    "    s = series\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.Series(np.where(s > 0, 1, np.where(s < 0, -1, 0)), index=s.index)\n",
    "    ss = s.astype(str).str.lower()\n",
    "    up = ss.str.contains(\"up\") | ss.str.fullmatch(\"u\") | ss.str.contains(\"↑\")\n",
    "    down = ss.str.contains(\"down\") | ss.str.fullmatch(\"d\") | ss.str.contains(\"↓\")\n",
    "    return pd.Series(np.where(up, 1, np.where(down, -1, 0)), index=s.index)\n",
    "\n",
    "def floor_entropy(values):\n",
    "    v = pd.Series(values).dropna()\n",
    "    if len(v) == 0:\n",
    "        return 0.0\n",
    "    counts = v.value_counts()\n",
    "    p = counts / counts.sum()\n",
    "    ent = -(p * np.log(p + 1e-12)).sum()\n",
    "    return float(ent / (np.log(len(counts) + 1e-12)))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load + standardize times\n",
    "# -----------------------------\n",
    "def load_data():\n",
    "    hall = pd.read_csv(PATH_HALL)\n",
    "    stop = pd.read_csv(PATH_STOP)\n",
    "    dep  = pd.read_csv(PATH_DEP)\n",
    "    load = pd.read_csv(PATH_LOAD)\n",
    "    maint= pd.read_csv(PATH_MAINT)\n",
    "\n",
    "    for df in [hall, stop, dep, load, maint]:\n",
    "        tcol = pick_col(df, [\"Time\", \"time\", \"timestamp\", \"Datetime\", \"DateTime\"], required=True)\n",
    "        df[\"Time\"] = pd.to_datetime(df[tcol])\n",
    "\n",
    "    return hall, stop, dep, load, maint\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Build 5-min features (Task2-style) for mode clustering\n",
    "# -----------------------------\n",
    "def build_5min_features(freq=DECISION_FREQ):\n",
    "    hall, stop, dep, load, maint = load_data()\n",
    "\n",
    "    # Hall calls\n",
    "    h_dir = pick_col(hall, [\"Direction\",\"direction\",\"CallDirection\",\"HallDirection\",\"UpDown\",\"dir\"])\n",
    "    h_floor = pick_col(hall, [\"Floor\",\"floor\",\"OriginFloor\",\"FromFloor\",\"HallFloor\",\"StartFloor\"], required=True)\n",
    "    h = hall.set_index(\"Time\")\n",
    "\n",
    "    hall_calls = h.resample(freq).size().rename(\"hall_calls\")\n",
    "    if h_dir:\n",
    "        d = normalize_direction(h[h_dir])\n",
    "        hall_up = (d == 1).resample(freq).sum().rename(\"hall_up\")\n",
    "        hall_down = (d == -1).resample(freq).sum().rename(\"hall_down\")\n",
    "    else:\n",
    "        hall_up = pd.Series(index=hall_calls.index, data=0.0, name=\"hall_up\")\n",
    "        hall_down = pd.Series(index=hall_calls.index, data=0.0, name=\"hall_down\")\n",
    "\n",
    "    hall_ent = h[h_floor].resample(freq).apply(floor_entropy).rename(\"origin_entropy\")\n",
    "\n",
    "    # Stops\n",
    "    s = stop.set_index(\"Time\")\n",
    "    s_floor = pick_col(stop, [\"Floor\",\"floor\",\"StopFloor\",\"AtFloor\",\"CurrentFloor\"])\n",
    "    stops = s.resample(freq).size().rename(\"stops\")\n",
    "    if s_floor:\n",
    "        # \"stop entropy\" can proxy spread of served floors\n",
    "        stop_ent = s[s_floor].resample(freq).apply(floor_entropy).rename(\"stop_entropy\")\n",
    "    else:\n",
    "        stop_ent = pd.Series(index=hall_calls.index, data=0.0, name=\"stop_entropy\")\n",
    "\n",
    "    # Departures\n",
    "    d = dep.set_index(\"Time\")\n",
    "    departures = d.resample(freq).size().rename(\"departures\")\n",
    "\n",
    "    # Load changes\n",
    "    ld = load.set_index(\"Time\")\n",
    "    in_col = pick_col(load, [\"Load In\",\"LoadIn\",\"load_in\",\"InLoad\",\"WeightIn\"])\n",
    "    out_col= pick_col(load, [\"Load Out\",\"LoadOut\",\"load_out\",\"OutLoad\",\"WeightOut\"])\n",
    "    load_in = ld[in_col].resample(freq).sum().fillna(0).rename(\"load_in\") if in_col else pd.Series(index=hall_calls.index, data=0.0, name=\"load_in\")\n",
    "    load_out= ld[out_col].resample(freq).sum().fillna(0).rename(\"load_out\") if out_col else pd.Series(index=hall_calls.index, data=0.0, name=\"load_out\")\n",
    "\n",
    "    # Maintenance ratio (rough)\n",
    "    m = maint.set_index(\"Time\")\n",
    "    m_flag = pick_col(maint, [\"Maintenance\",\"maintenance\",\"IsMaintenance\",\"MaintMode\",\"Mode\"])\n",
    "    m_eid  = pick_col(maint, [\"Elevator\",\"elevator\",\"CarID\",\"CarId\",\"LiftID\",\"ID\"])\n",
    "    if m_flag and m_eid:\n",
    "        mm = m.copy()\n",
    "        mm[\"maint_flag\"] = (\n",
    "            mm[m_flag].astype(int) if pd.api.types.is_numeric_dtype(mm[m_flag])\n",
    "            else mm[m_flag].astype(str).str.lower().isin([\"1\",\"true\",\"yes\",\"maint\",\"maintenance\"]).astype(int)\n",
    "        )\n",
    "        maint_ratio = (\n",
    "            mm.groupby([pd.Grouper(freq=freq), m_eid])[\"maint_flag\"].last()\n",
    "              .groupby(level=0).mean()\n",
    "              .reindex(hall_calls.index).fillna(method=\"ffill\").fillna(0)\n",
    "              .rename(\"maint_ratio\")\n",
    "        )\n",
    "    else:\n",
    "        maint_ratio = pd.Series(index=hall_calls.index, data=0.0, name=\"maint_ratio\")\n",
    "\n",
    "    feats = pd.concat(\n",
    "        [\n",
    "            hall_calls,\n",
    "            hall_up.reindex(hall_calls.index).fillna(0),\n",
    "            hall_down.reindex(hall_calls.index).fillna(0),\n",
    "            hall_ent.reindex(hall_calls.index).fillna(0),\n",
    "            stops.reindex(hall_calls.index).fillna(0),\n",
    "            stop_ent.reindex(hall_calls.index).fillna(0),\n",
    "            departures.reindex(hall_calls.index).fillna(0),\n",
    "            load_in.reindex(hall_calls.index).fillna(0),\n",
    "            load_out.reindex(hall_calls.index).fillna(0),\n",
    "            maint_ratio.reindex(hall_calls.index).fillna(0),\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    total = feats[\"hall_calls\"].replace(0, np.nan)\n",
    "    feats[\"up_ratio\"] = (feats[\"hall_up\"] / total).fillna(0)\n",
    "    feats[\"down_ratio\"] = (feats[\"hall_down\"] / total).fillna(0)\n",
    "\n",
    "    # time cyc features\n",
    "    t = feats.index\n",
    "    minute_of_day = t.hour * 60 + t.minute\n",
    "    feats[\"sin_day\"] = np.sin(2*np.pi*minute_of_day/1440.0)\n",
    "    feats[\"cos_day\"] = np.cos(2*np.pi*minute_of_day/1440.0)\n",
    "\n",
    "    feats[\"activity\"] = feats[\"hall_calls\"] + 0.5*feats[\"stops\"] + 0.2*feats[\"departures\"]\n",
    "\n",
    "    feats = feats.reset_index().rename(columns={\"index\":\"Time\"})\n",
    "    feats[\"Time\"] = pd.to_datetime(feats[\"Time\"])\n",
    "    return feats\n",
    "\n",
    "def train_mode_cluster(feats: pd.DataFrame, n_clusters=N_CLUSTERS):\n",
    "    feature_cols = [\n",
    "        \"hall_calls\",\"up_ratio\",\"down_ratio\",\"origin_entropy\",\n",
    "        \"stops\",\"stop_entropy\",\"departures\",\n",
    "        \"load_in\",\"load_out\",\"maint_ratio\",\n",
    "        \"sin_day\",\"cos_day\",\"activity\"\n",
    "    ]\n",
    "    X = feats[feature_cols].fillna(0.0).values\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=42, n_init=20))\n",
    "    ])\n",
    "    pipe.fit(X)\n",
    "    feats = feats.copy()\n",
    "    feats[\"cluster\"] = pipe.predict(X)\n",
    "    return pipe, feature_cols, feats\n",
    "\n",
    "def label_clusters(feats_with_cluster: pd.DataFrame) -> Dict[int, str]:\n",
    "    \"\"\"Map cluster -> human-readable mode label (simple explainable rules).\"\"\"\n",
    "    g = feats_with_cluster.groupby(\"cluster\").mean(numeric_only=True)\n",
    "\n",
    "    mapping = {}\n",
    "    act_q25 = g[\"activity\"].quantile(0.25)\n",
    "    act_med = g[\"activity\"].median()\n",
    "\n",
    "    for k, row in g.iterrows():\n",
    "        activity = row[\"activity\"]\n",
    "        up = row[\"up_ratio\"]\n",
    "        down = row[\"down_ratio\"]\n",
    "        ent = row[\"origin_entropy\"]\n",
    "        stop_ent = row[\"stop_entropy\"]\n",
    "\n",
    "        if activity <= act_q25:\n",
    "            label = \"Idle/Low\"\n",
    "        elif up > 0.65 and activity >= act_med:\n",
    "            label = \"Up-Peak\"\n",
    "        elif down > 0.65 and activity >= act_med:\n",
    "            label = \"Down-Peak\"\n",
    "        elif ent > 0.70 or stop_ent > 0.70:\n",
    "            label = \"Inter-floor/Meeting\"\n",
    "        else:\n",
    "            label = \"Balanced/Meal-hour\"\n",
    "        mapping[int(k)] = label\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Learn demand weights by (mode, floor)\n",
    "# -----------------------------\n",
    "def learn_floor_demand_by_mode(hall: pd.DataFrame, feats_clustered: pd.DataFrame, freq=DECISION_FREQ):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      demand[(mode_label)][floor] = avg hall-call count per horizon window\n",
    "    \"\"\"\n",
    "    # Identify hall origin floor column\n",
    "    h_floor = pick_col(hall, [\"Floor\",\"floor\",\"OriginFloor\",\"FromFloor\",\"HallFloor\",\"StartFloor\"], required=True)\n",
    "\n",
    "    # Attach cluster/mode to each hall call by matching its 5-min slice\n",
    "    hall2 = hall.copy()\n",
    "    hall2[\"slice\"] = hall2[\"Time\"].dt.floor(freq)\n",
    "\n",
    "    cluster_map = feats_clustered.set_index(\"Time\")[\"cluster\"].to_dict()\n",
    "    hall2[\"cluster\"] = hall2[\"slice\"].map(cluster_map)\n",
    "\n",
    "    # Drop calls outside feature range\n",
    "    hall2 = hall2.dropna(subset=[\"cluster\"])\n",
    "    hall2[\"cluster\"] = hall2[\"cluster\"].astype(int)\n",
    "\n",
    "    # Count hall calls per (slice, cluster, floor)\n",
    "    counts = (\n",
    "        hall2.groupby([\"slice\",\"cluster\", hall2[h_floor]])[\"Time\"]\n",
    "             .size()\n",
    "             .rename(\"cnt\")\n",
    "             .reset_index()\n",
    "             .rename(columns={h_floor:\"floor\"})\n",
    "    )\n",
    "\n",
    "    # Average demand per slice for each cluster-floor\n",
    "    avg = counts.groupby([\"cluster\",\"floor\"])[\"cnt\"].mean().reset_index()\n",
    "\n",
    "    return avg  # columns: cluster, floor, cnt\n",
    "\n",
    "# -----------------------------\n",
    "# 4) k-median on 1D floors (choose parking floors for k idle elevators)\n",
    "# -----------------------------\n",
    "def weighted_k_median_1d(floors: np.ndarray, weights: np.ndarray, k: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Choose k facility locations on 1D line to minimize sum_i w_i * |x_i - facility(x_i)|.\n",
    "    Facilities must be at existing floor positions (integers).\n",
    "    DP solution O(k*n^2) for n unique floors, small n => OK.\n",
    "    \"\"\"\n",
    "    # Sort\n",
    "    order = np.argsort(floors)\n",
    "    x = floors[order].astype(int)\n",
    "    w = weights[order].astype(float)\n",
    "    n = len(x)\n",
    "\n",
    "    # Precompute cost[i][j] = cost of serving points i..j with 1 median facility\n",
    "    # Median index m is weighted median. For simplicity (n small), brute find best median.\n",
    "    cost = np.zeros((n, n))\n",
    "    best_m = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            idx = np.arange(i, j+1)\n",
    "            # find best median among idx\n",
    "            best = None\n",
    "            best_idx = i\n",
    "            for m in idx:\n",
    "                c = np.sum(w[idx] * np.abs(x[idx] - x[m]))\n",
    "                if best is None or c < best:\n",
    "                    best = c\n",
    "                    best_idx = m\n",
    "            cost[i, j] = best if best is not None else 0.0\n",
    "            best_m[i, j] = best_idx\n",
    "\n",
    "    # DP: dp[t][j] = min cost using t facilities to cover points 0..j\n",
    "    dp = np.full((k+1, n), np.inf)\n",
    "    prev = np.full((k+1, n), -1, dtype=int)\n",
    "\n",
    "    # base: 1 facility\n",
    "    for j in range(n):\n",
    "        dp[1, j] = cost[0, j]\n",
    "        prev[1, j] = -1\n",
    "\n",
    "    for t in range(2, k+1):\n",
    "        for j in range(n):\n",
    "            # split at p: cover 0..p with t-1, and p+1..j with 1\n",
    "            for p in range(t-2, j):\n",
    "                cand = dp[t-1, p] + cost[p+1, j]\n",
    "                if cand < dp[t, j]:\n",
    "                    dp[t, j] = cand\n",
    "                    prev[t, j] = p\n",
    "\n",
    "    # Recover segments\n",
    "    facilities = []\n",
    "    t = k\n",
    "    j = n-1\n",
    "    while t >= 1 and j >= 0:\n",
    "        p = prev[t, j]\n",
    "        i = 0 if p == -1 else p+1\n",
    "        m = best_m[i, j]\n",
    "        facilities.append(int(x[m]))\n",
    "        j = p\n",
    "        t -= 1\n",
    "\n",
    "    facilities.reverse()\n",
    "    return facilities\n",
    "\n",
    "def assign_elevators_to_targets(elevator_floors: Dict[int,int], targets: List[int]) -> Dict[int,int]:\n",
    "    \"\"\"\n",
    "    Greedy matching: assign each target to closest available elevator.\n",
    "    Returns: elevator_id -> target_floor\n",
    "    \"\"\"\n",
    "    eids = list(elevator_floors.keys())\n",
    "    remaining = set(eids)\n",
    "    assignment = {}\n",
    "\n",
    "    for tgt in targets:\n",
    "        best_e, best_d = None, None\n",
    "        for e in list(remaining):\n",
    "            d = abs(elevator_floors[e] - tgt)\n",
    "            if best_d is None or d < best_d:\n",
    "                best_d = d\n",
    "                best_e = e\n",
    "        if best_e is None:\n",
    "            break\n",
    "        assignment[best_e] = tgt\n",
    "        remaining.remove(best_e)\n",
    "\n",
    "    # Any leftover elevators (if more idle than targets): keep them where they are\n",
    "    for e in remaining:\n",
    "        assignment[e] = elevator_floors[e]\n",
    "    return assignment\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Lightweight event simulator for waiting time\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class ElevatorState:\n",
    "    floor: int\n",
    "    available_time: pd.Timestamp  # when elevator can take new call\n",
    "\n",
    "def travel_time_seconds(f1: int, f2: int) -> float:\n",
    "    return abs(int(f1) - int(f2)) * SECONDS_PER_FLOOR\n",
    "\n",
    "def infer_initial_positions(stop: pd.DataFrame, start_time: pd.Timestamp) -> Dict[int,int]:\n",
    "    \"\"\"\n",
    "    Infer elevator last known floor before start_time from car_stops.\n",
    "    Falls back to lobby if unknown.\n",
    "    \"\"\"\n",
    "    eid_col = pick_col(stop, [\"Elevator\",\"elevator\",\"CarID\",\"CarId\",\"LiftID\",\"ID\"], required=False)\n",
    "    floor_col = pick_col(stop, [\"Floor\",\"floor\",\"StopFloor\",\"AtFloor\",\"CurrentFloor\"], required=False)\n",
    "\n",
    "    pos = {i: LOBBY_FLOOR for i in range(N_ELEVATORS)}\n",
    "    if eid_col is None or floor_col is None:\n",
    "        return pos\n",
    "\n",
    "    s = stop[stop[\"Time\"] <= start_time].copy()\n",
    "    if s.empty:\n",
    "        return pos\n",
    "\n",
    "    s = s.sort_values(\"Time\")\n",
    "    last = s.groupby(eid_col).tail(1)\n",
    "    for _, r in last.iterrows():\n",
    "        try:\n",
    "            eid = int(r[eid_col])\n",
    "            pos[eid] = int(r[floor_col])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pos\n",
    "\n",
    "def simulate(strategy: str,\n",
    "             hall: pd.DataFrame,\n",
    "             feats: pd.DataFrame,\n",
    "             mode_pipe: Pipeline,\n",
    "             mode_features: List[str],\n",
    "             cluster_to_label: Dict[int,str],\n",
    "             demand_by_cluster_floor: pd.DataFrame,\n",
    "             start_time: Optional[pd.Timestamp]=None,\n",
    "             end_time: Optional[pd.Timestamp]=None) -> Dict[str,float]:\n",
    "    \"\"\"\n",
    "    strategy in {\"last_stop\",\"lobby\",\"dynamic\"}.\n",
    "    \"\"\"\n",
    "    # columns\n",
    "    h_floor = pick_col(hall, [\"Floor\",\"floor\",\"OriginFloor\",\"FromFloor\",\"HallFloor\",\"StartFloor\"], required=True)\n",
    "\n",
    "    hall = hall.sort_values(\"Time\").copy()\n",
    "    if start_time is None:\n",
    "        start_time = hall[\"Time\"].min()\n",
    "    if end_time is None:\n",
    "        end_time = hall[\"Time\"].max()\n",
    "\n",
    "    # Filter simulation interval\n",
    "    hall_sim = hall[(hall[\"Time\"] >= start_time) & (hall[\"Time\"] <= end_time)].copy()\n",
    "    if hall_sim.empty:\n",
    "        return {\"AWT\": np.nan, \"LongWaitPct\": np.nan, \"N\": 0}\n",
    "\n",
    "    # Initial elevator states\n",
    "    # (use last stop before start_time if possible; otherwise lobby)\n",
    "    _, stop, _, _, _ = load_data()\n",
    "    init_pos = infer_initial_positions(stop, start_time)\n",
    "\n",
    "    elevators = {\n",
    "        e: ElevatorState(floor=init_pos.get(e, LOBBY_FLOOR), available_time=start_time)\n",
    "        for e in range(N_ELEVATORS)\n",
    "    }\n",
    "\n",
    "    # Precompute demand lookup: (cluster -> arrays floors, weights)\n",
    "    demand_group = demand_by_cluster_floor.groupby(\"cluster\")\n",
    "    demand_lookup = {}\n",
    "    for cl, dfc in demand_group:\n",
    "        floors = dfc[\"floor\"].astype(int).values\n",
    "        w = dfc[\"cnt\"].astype(float).values\n",
    "        # avoid all-zero\n",
    "        if w.sum() <= 0:\n",
    "            w = np.ones_like(w)\n",
    "        demand_lookup[int(cl)] = (floors, w)\n",
    "\n",
    "    waits = []\n",
    "\n",
    "    # Decision times for parking\n",
    "    decision_times = pd.date_range(start=start_time.floor(DECISION_FREQ),\n",
    "                                   end=end_time.ceil(DECISION_FREQ),\n",
    "                                   freq=DECISION_FREQ)\n",
    "\n",
    "    # Helper to get cluster at a decision time (nearest available feature row)\n",
    "    feats_idx = feats.set_index(\"Time\").sort_index()\n",
    "\n",
    "    def get_cluster_label_at(t: pd.Timestamp) -> Tuple[int,str]:\n",
    "        # use exact row if exists, else previous\n",
    "        tt = t.floor(DECISION_FREQ)\n",
    "        if tt in feats_idx.index:\n",
    "            row = feats_idx.loc[tt]\n",
    "        else:\n",
    "            # previous available\n",
    "            row = feats_idx.loc[:tt].tail(1)\n",
    "            if isinstance(row, pd.DataFrame):\n",
    "                row = row.iloc[0]\n",
    "        X = row[mode_features].fillna(0.0).values.reshape(1, -1)\n",
    "        cl = int(mode_pipe.predict(X)[0])\n",
    "        return cl, cluster_to_label.get(cl, \"Unknown\")\n",
    "\n",
    "    # Parking policy invoked at decision times\n",
    "    def apply_parking_policy(t: pd.Timestamp):\n",
    "        # Identify idle elevators (available_time <= t)\n",
    "        idle = [e for e, st in elevators.items() if st.available_time <= t]\n",
    "\n",
    "        if len(idle) == 0:\n",
    "            return\n",
    "\n",
    "        if strategy == \"last_stop\":\n",
    "            return\n",
    "\n",
    "        if strategy == \"lobby\":\n",
    "            for e in idle:\n",
    "                st = elevators[e]\n",
    "                if st.floor != LOBBY_FLOOR:\n",
    "                    tt = travel_time_seconds(st.floor, LOBBY_FLOOR)\n",
    "                    st.available_time = t + pd.Timedelta(seconds=tt)\n",
    "                    st.floor = LOBBY_FLOOR\n",
    "            return\n",
    "\n",
    "        if strategy == \"dynamic\":\n",
    "            cl, _ = get_cluster_label_at(t)\n",
    "\n",
    "            # demand for next horizon: scale weights by horizon slices\n",
    "            if cl in demand_lookup:\n",
    "                floors, w = demand_lookup[cl]\n",
    "            else:\n",
    "                # fallback to global\n",
    "                allf = demand_by_cluster_floor.groupby(\"floor\")[\"cnt\"].mean().reset_index()\n",
    "                floors = allf[\"floor\"].astype(int).values\n",
    "                w = allf[\"cnt\"].astype(float).values\n",
    "                if w.sum() <= 0:\n",
    "                    w = np.ones_like(w)\n",
    "\n",
    "            # Choose k target parking floors for number of idle elevators\n",
    "            k = min(len(idle), N_ELEVATORS)\n",
    "            # Reduce to unique floors if too many\n",
    "            if len(floors) == 0:\n",
    "                targets = [LOBBY_FLOOR] * k\n",
    "            else:\n",
    "                # If floors extremely many, keep top by weight to stabilize DP\n",
    "                if len(floors) > 60:\n",
    "                    top_idx = np.argsort(-w)[:60]\n",
    "                    floors2, w2 = floors[top_idx], w[top_idx]\n",
    "                else:\n",
    "                    floors2, w2 = floors, w\n",
    "\n",
    "                # normalize weights\n",
    "                w2 = w2 / (w2.sum() + 1e-12)\n",
    "\n",
    "                # Compute k-median targets\n",
    "                targets = weighted_k_median_1d(floors2, w2, k)\n",
    "\n",
    "            # Assign idle elevators to targets\n",
    "            cur_pos = {e: elevators[e].floor for e in idle}\n",
    "            assignment = assign_elevators_to_targets(cur_pos, targets)\n",
    "\n",
    "            for e, tgt in assignment.items():\n",
    "                st = elevators[e]\n",
    "                if st.floor != tgt:\n",
    "                    tt = travel_time_seconds(st.floor, tgt)\n",
    "                    st.available_time = t + pd.Timedelta(seconds=tt)\n",
    "                    st.floor = tgt\n",
    "\n",
    "    # Simulation loop over calls; we also apply parking at decision points\n",
    "    decision_ptr = 0\n",
    "    decision_times = list(decision_times)\n",
    "\n",
    "    for _, call in hall_sim.iterrows():\n",
    "        call_time = call[\"Time\"]\n",
    "        origin_floor = int(call[h_floor])\n",
    "\n",
    "        # Apply parking decisions up to this call_time\n",
    "        while decision_ptr < len(decision_times) and decision_times[decision_ptr] <= call_time:\n",
    "            apply_parking_policy(decision_times[decision_ptr])\n",
    "            decision_ptr += 1\n",
    "\n",
    "        # Choose an elevator to serve this call:\n",
    "        # minimize arrival time = max(available_time, call_time) + travel_time(current_floor -> origin)\n",
    "        best_e = None\n",
    "        best_arrival = None\n",
    "\n",
    "        for e, st in elevators.items():\n",
    "            start_service = max(st.available_time, call_time)\n",
    "            arr = start_service + pd.Timedelta(seconds=travel_time_seconds(st.floor, origin_floor))\n",
    "            if best_arrival is None or arr < best_arrival:\n",
    "                best_arrival = arr\n",
    "                best_e = e\n",
    "\n",
    "        # Waiting time: arrival - call_time (doors open assumed at arrival + DOOR_TIME doesn't affect waiting-to-arrival)\n",
    "        wait_sec = (best_arrival - call_time).total_seconds()\n",
    "        waits.append(wait_sec)\n",
    "\n",
    "        # Update chosen elevator state: after serving, elevator stays at origin floor, becomes available after door time\n",
    "        st = elevators[best_e]\n",
    "        st.floor = origin_floor\n",
    "        st.available_time = best_arrival + pd.Timedelta(seconds=DOOR_TIME)\n",
    "\n",
    "    waits = np.array(waits, dtype=float)\n",
    "    awt = float(np.mean(waits)) if len(waits) else np.nan\n",
    "    long_pct = float(np.mean(waits >= LONG_WAIT_THRESHOLD) * 100.0) if len(waits) else np.nan\n",
    "\n",
    "    return {\"AWT\": awt, \"LongWaitPct\": long_pct, \"N\": int(len(waits))}\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Train everything + run comparison\n",
    "# -----------------------------\n",
    "def run_task3_demo():\n",
    "    hall, _, _, _, _ = load_data()\n",
    "\n",
    "    # (A) Build mode features + cluster model\n",
    "    feats = build_5min_features()\n",
    "    mode_pipe, mode_features, feats_clustered = train_mode_cluster(feats, n_clusters=N_CLUSTERS)\n",
    "    cl_to_label = label_clusters(feats_clustered)\n",
    "\n",
    "    # (B) Learn demand by (cluster, floor)\n",
    "    demand = learn_floor_demand_by_mode(hall, feats_clustered)\n",
    "\n",
    "    # (C) Choose a simulation window (e.g., last 3 days) to keep runtime reasonable\n",
    "    # You can expand to full 30 days once happy.\n",
    "    tmin = hall[\"Time\"].min()\n",
    "    tmax = hall[\"Time\"].max()\n",
    "    sim_start = max(tmin, tmax - pd.Timedelta(days=3))\n",
    "    sim_end = tmax\n",
    "\n",
    "    # (D) Evaluate strategies\n",
    "    results = {}\n",
    "    for strat in [\"last_stop\", \"lobby\", \"dynamic\"]:\n",
    "        res = simulate(\n",
    "            strategy=strat,\n",
    "            hall=hall,\n",
    "            feats=feats_clustered,\n",
    "            mode_pipe=mode_pipe,\n",
    "            mode_features=mode_features,\n",
    "            cluster_to_label=cl_to_label,\n",
    "            demand_by_cluster_floor=demand,\n",
    "            start_time=sim_start,\n",
    "            end_time=sim_end\n",
    "        )\n",
    "        results[strat] = res\n",
    "\n",
    "    print(\"\\n========== Task3 Strategy Comparison ==========\")\n",
    "    print(f\"Simulation window: {sim_start}  ->  {sim_end}\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k:>10} | AWT={v['AWT']:.2f}s | LongWait%={v['LongWaitPct']:.2f}% | N={v['N']}\")\n",
    "\n",
    "    # Save artifacts useful for report\n",
    "    feats_clustered.to_csv(str(OUT_DATA / \"task3_mode_features_5min.csv\"), index=False)\n",
    "    demand.to_csv(str(OUT_DATA / \"task3_demand_cluster_floor.csv\"), index=False)\n",
    "\n",
    "    \n",
    "    # Save simulation summary for report tables\n",
    "    try:\n",
    "        sim_rows = []\n",
    "        for strat, v in results.items():\n",
    "            sim_rows.append({\n",
    "                \"strategy\": strat,\n",
    "                \"AWT_sec\": float(v.get(\"AWT\", np.nan)),\n",
    "                \"LongWait_pct\": float(v.get(\"LongWaitPct\", np.nan)),\n",
    "                \"N_calls\": int(v.get(\"N\", 0))\n",
    "            })\n",
    "        sim_df = pd.DataFrame(sim_rows)\n",
    "        sim_out = str(OUT_DATA / \"task3_sim_results.csv\")\n",
    "        sim_df.to_csv(sim_out, index=False)\n",
    "        print(f\"Saved simulation summary to: {sim_out}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Task3 sim export failed:\", e)\n",
    "\n",
    "return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_task3_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047ada75-bc5a-43be-a7ea-c1a3d789342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_pred_series.csv OK shape= (1293, 3) cols= ['Time', 'y_true', 'y_pred']\n",
      "task2_modes_timeline.csv MISSING\n",
      "task3_mode_features_5min.csv MISSING\n",
      "task3_demand_cluster_floor.csv MISSING\n",
      "task3_sim_results.csv MISSING\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\n",
    "    OUT_DATA / \"task1_pred_series.csv\",\n",
    "    OUT_DATA / \"task2_modes_timeline.csv\",\n",
    "    OUT_DATA / \"task3_mode_features_5min.csv\",\n",
    "    OUT_DATA / \"task3_demand_cluster_floor.csv\",\n",
    "    OUT_DATA / \"task3_sim_results.csv\",\n",
    "]\n",
    "\n",
    "for p in files:\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        print(p.name, \"OK\", \"shape=\", df.shape, \"cols=\", df.columns.tolist()[:10])\n",
    "    else:\n",
    "        print(p.name, \"MISSING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db412cb-328a-4c3e-b7c4-43f452598697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
